{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep research of electrocardiograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous tutorials you have learned about batches, pipelines and models. All those concepts are essential components of deep learning research, and CardIO provides convenient implementations of them for all users.\n",
    "But in order to perform real research, you need to conduct well-described, reproducible experiments and keep track on the results.\n",
    "\n",
    "For this task `research` module of `batchflow` comes in handy. In this tutorial we will use `research` to train and test multiple variations of ResNet architecture in the task of atrial fibrillation classification.\n",
    "\n",
    "In this tutorial we will cover part of the functionality of `research`, but we strongly recommend to go through [BatchFlow's Research tutorial](https://github.com/analysiscenter/batchflow/blob/master/examples/tutorials/08_research.ipynb) and to take a look at [documentation for Research](https://analysiscenter.github.io/batchflow/intro/research.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Experimental design](#Experimetal-design)\n",
    "    * [Describing model variations](#Describing-model-variations)\n",
    "    * [Creating dataset](#Creating-dataset)\n",
    "    * [Setting up model configuration](#Setting-up-model-configuration)\n",
    "    * [Setting up training parameters](#Setting-up-training-parameters)\n",
    "    * [Defining pipelines](#Defining-pipelines)\n",
    "* [Running the experiment](#Running-the-experiment)\n",
    "* [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are starting an experiment, the very first thing you should do is to write down the experimental design.  Clear and explicit design of experiment makes it easier to reproduce the results, because it helps to make sure that all important conditions are accounted.\n",
    "\n",
    "We will now follow this rule and step by step generate experimental desing, which later will be used as configuration for our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing model variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we shall define which variations of ResNet architecture we are going to test.\n",
    "`ResNet` class from `batchflow.models` has following configuration parameters:\n",
    "* number of filters after first convolution\n",
    "* layout in ResNet blocks\n",
    "* number of filters in ResNet blocks' convolutions\n",
    "* number of ResNet blocks between poolings\n",
    "\n",
    "We are going to tweak all of them! And, we suppose that you are already familiar with models from `batchflow.models`.\n",
    "\n",
    "\n",
    "Along with custom architectures, let's try some classics, like `ResNet18` and `ResNet34`. To do this, we define \"model\" option of our experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from cardio.batchflow.models.tf import ResNet, ResNet18, ResNet34\n",
    "from cardio.batchflow.research import Option, Grid, Research\n",
    "\n",
    "model_op1 = Option('model', [ResNet18, ResNet34])\n",
    "model_op2 = Option('model', [ResNet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have defined custom and classic ResNets in different options. Have some patience, in a few moments we will explain this move.\n",
    "\n",
    "Next, let's define options for input filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filters_op1 = Option('input_filters', [64])\n",
    "input_filters_op2 = Option('input_filters', [32, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now just follow the same procedure for the rest of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_op1 = Option('layout', ['cnacna'])\n",
    "layout_op2 = Option('layout', ['cna', 'cnacna'])\n",
    "\n",
    "blocks_op1= Option('blocks', [[2, 2, 2, 2], [3, 4, 6, 3]])\n",
    "blocks_op2 = Option('blocks', [[2, 3, 4, 5, 4, 3, 2], [2, 2, 2, 2, 2, 2, 2],\n",
    "                               [1, 1, 1, 1, 1, 1, 1]])\n",
    "\n",
    "filters_op1 = Option('filters', [[64, 128, 256, 512], [64, 128, 256, 512]])\n",
    "filters_op2 = Option('filters', [[4, 8, 16, 32, 64, 128, 256], [4, 4, 8, 8, 16, 16, 20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just the time to explain why we define two options for each parameter.\n",
    "\n",
    "We have defined options for different parameters, but what are we going to do with them now? We will use them to generate a single grid of parameters, that will include all the combinations we want to test. \n",
    "\n",
    "Here, take a look at this simple example to get the intuition about grid generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grid([[{'p1': ['1', '2']}, {'p2': ['v1', 'v2']}], [{'p1': ['4']}, {'p2': ['v3']}]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op1 = Option('p1', [1, 2])\n",
    "op2 = Option('p2', ['v1', 'v2'])\n",
    "op3 = Option('p1', [4])\n",
    "op4 = Option('p2', ['v3'])\n",
    "\n",
    "grid = (op1 * op2 + op3 * op4)\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can perform multipliction and addition of options. But, as far as `Grid` object is not very intuitive, we can generate all configurations from this grid and take a look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ConfigAlias({'p1': '1', 'p2': 'v1'}),\n",
       " ConfigAlias({'p1': '1', 'p2': 'v2'}),\n",
       " ConfigAlias({'p1': '2', 'p2': 'v1'}),\n",
       " ConfigAlias({'p1': '2', 'p2': 'v2'}),\n",
       " ConfigAlias({'p1': '4', 'p2': 'v3'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(grid.gen_configs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This form is much simpler to understand! \n",
    "\n",
    "Also, there is a method `Option.product` that implements element-wise multiplication - see an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ConfigAlias({'p1': '1', 'p2': 'v1'}), ConfigAlias({'p1': '2', 'p2': 'v2'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = Option.product(op1, op2)\n",
    "    \n",
    "list(grid.gen_configs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to generate the grid for our experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = (Option.product(model_op1, blocks_op1, filters_op1) * layout_op1 * input_filters_op1 + \n",
    "        model_op2 * layout_op2 * input_filters_op2 * blocks_op2 * filters_op2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we're done with model variations. Now we shall move to the next part - the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just follow familiar procedure: setting up paths to the signals and the labels, then creating dataset instance and splitting data into train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cardio import batchflow as bf\n",
    "from cardio import EcgDataset\n",
    "\n",
    "PATH = \"/notebooks/data/ECG/training2017\" # Change this path for your data dicrectory\n",
    "SIGNALS_MASK = os.path.join(PATH, \"A*.hea\")\n",
    "LABELS_PATH = os.path.join(PATH, \"REFERENCE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eds = EcgDataset(path=SIGNALS_MASK, no_ext=True, sort=True)\n",
    "eds.split(0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here comes the interesting part! When we define `model_config`, we use `C('parameter_name')` for all the parameters we want to vary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from cardio.batchflow import F, B, C, V\n",
    "\n",
    "model_config = {\n",
    "    'inputs': dict(signals={'shape': F(lambda batch: batch.signal[0].shape[1:])},\n",
    "                   labels={'classes': ['A', 'NO'], 'transform': 'ohe', 'name': 'targets'}),\n",
    "    'initial_block/inputs': 'signals',\n",
    "    \"loss\": \"ce\",\n",
    "    \"input_block/filters\": C('input_filters'),\n",
    "    \"body/block/layout\": C('layout'),\n",
    "    \"body/filters\": C('filters'),\n",
    "    \"body/num_blocks\": C('blocks'),\n",
    "    \"session/config\": tf.ConfigProto(allow_soft_placement=True),\n",
    "    \"device\": C(\"device\"),\n",
    "    \"optimizer\": \"Adam\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corresponding values from configuration will be inserted instead of those templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is pretty short and simple - just setting up batch size and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we define root pipelines, wich contain only signal processing. Also, we will define a function to flip signals whose R peaks are turned down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "\n",
    "@njit(nogil=True)\n",
    "def center_flip(signal):\n",
    "    \"\"\"\n",
    "    Center signal and flip signals with R peaks turned down.\n",
    "    \"\"\"\n",
    "    return np.random.choice(np.array([1, -1])) * (signal - np.mean(signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_train = (\n",
    "  bf.Pipeline()\n",
    "    .load(components=[\"signal\", \"meta\"], fmt=\"wfdb\")\n",
    "    .load(components=\"target\", fmt=\"csv\", src=LABELS_PATH)\n",
    "    .drop_labels([\"~\"])\n",
    "    .rename_labels({\"N\": \"NO\", \"O\": \"NO\"})\n",
    "    .apply_to_each_channel(center_flip)\n",
    "    .random_resample_signals(\"normal\", loc=300, scale=10)\n",
    "    .random_split_signals(3000, {\"A\": 6, \"NO\": 2})\n",
    "    .apply_transform(func=np.transpose, src='signal', dst='signal', axes=[0, 2, 1])\n",
    ").run(BATCH_SIZE, shuffle=True, drop_last=True, n_epochs=None, lazy=True)\n",
    "\n",
    "root_test = (\n",
    "  bf.Pipeline()\n",
    "    .load(components=[\"signal\", \"meta\"], fmt=\"wfdb\")\n",
    "    .load(components=\"target\", fmt=\"csv\", src=LABELS_PATH)\n",
    "    .drop_labels([\"~\"])\n",
    "    .rename_labels({\"N\": \"NO\", \"O\": \"NO\"})\n",
    "    .apply_to_each_channel(center_flip)\n",
    "    .split_signals(3000, 3000)\n",
    "    .apply_transform(func=np.transpose, src='signal', dst='signal', axes=[0, 2, 1])\n",
    ").run(BATCH_SIZE, shuffle=True, drop_last=True, n_epochs=1, lazy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define main pipelines. Training pipeline is pretty simlpe - we just train the model and save loss to pipeline variable.\n",
    "Again, we need to define simple function - this one prepares data from batch to be fed into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(batch, **kwagrs):\n",
    "    \"\"\"\n",
    "    Prepare data from `signal` and `target` component of batch to be fed into network.\n",
    "    Separates each crop as individual signal and makes a corresponding label.\n",
    "    \"\"\"\n",
    "    n_reps = [signal.shape[0] for signal in batch.signal]\n",
    "    signals = np.array([segment for signal in batch.signal for segment in signal])\n",
    "    targets = np.repeat(batch.target, n_reps, axis=0)\n",
    "    return {\"feed_dict\": {'signals': signals, 'labels': targets}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train = (\n",
    "  bf.Pipeline()\n",
    "    .init_variable('loss', init_on_each_run=list)\n",
    "    .init_model('dynamic', C('model'), 'model', config=model_config)\n",
    "    .train_model('model',\n",
    "                 make_data=make_data,\n",
    "                 fetches=[\"loss\"],\n",
    "                 save_to=[V(\"loss\")], mode=\"w\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing pipeline is a bit more complex. At first, we initialize a variable to store metrics and save number of splits for each signal in batch component `splits`. Then, as usual, import model, make predictions and save predictions and targets into the batch components.\n",
    "\n",
    "As far as we make predictions for splits, we need to aggrageate them to obtain prediction for the whole signal. To do so, we will use `aggregate_crop_predictions` and update corresponding batch components.\n",
    "Then, pipeline method `gather_metrics` accumulates targets and predictions from batch and allows to calculate metrics afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_crops(arr, splits, agg_func, predictions=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Aggregates predictions or labels from separate crops for whole signal.\n",
    "    If predictions is True, applies softmax function before aggreagation.\n",
    "    \"\"\"\n",
    "    if predictions:\n",
    "        arr -= np.max(arr, axis=1, keepdims=True)\n",
    "        arr_exp = np.exp(arr)\n",
    "        arr = (arr_exp / np.sum(arr_exp, axis=1, keepdims=True))\n",
    "    arr = np.split(arr, np.cumsum(splits)[:-1])\n",
    "    return np.array([agg_func(sig[:, 0]) for sig in arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = (\n",
    "  bf.Pipeline()\n",
    "    .init_variable(\"metrics\", init_on_each_run=None)\n",
    "    .apply_transform(src=\"signal\", dst=\"splits\", func=lambda x: [x.shape[0]])\n",
    "    .import_model(\"model\", C(\"import_from\"))\n",
    "    .predict_model(\"model\", make_data=make_data,\n",
    "                   fetches=[\"predictions\", \"targets\"], \n",
    "                   save_to=[B('predictions'), B(\"targets\")])\n",
    "    .apply_transform_all(func=aggregate_crops, predictions=True,\n",
    "                         src='predictions', dst='predictions', \n",
    "                         splits=B('splits'), agg_func=np.mean)\n",
    "    .apply_transform_all(func=aggregate_crops, predictions=False, \n",
    "                         src='targets', dst='targets',\n",
    "                         splits=B('splits'), agg_func=np.max)\n",
    "    .gather_metrics('class', targets=B('targets'), predictions=B('predictions'),\n",
    "                    fmt='proba', save_to=V('metrics'), mode='a')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the experiment, we have to set how often to run test pipeline. Calculation of this value is not very strainghtforward because it should be stated in a number of iterations, not epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_EACH_EPOCH = 20\n",
    "TRAIN_SIZE = len(eds.train)\n",
    "ITERATIONS = ((TRAIN_SIZE // BATCH_SIZE) + 1) * EPOCHS\n",
    "TEST_EXEC_FOR = ITERATIONS // EPOCHS * TEST_EACH_EPOCH\n",
    "STR_EXEC = '%{}'.format(TEST_EXEC_FOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we will set a few constants for research:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_REPS = 5\n",
    "N_BRANCHES = 2\n",
    "N_WORKERS = 3\n",
    "GPU = [1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of run of any pipeline we can execute some function. So let's write two functions to save number of trainable parameters and F1 score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainable_variables(iteration, experiment, ppl, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Returns a number of trainable variables in the model.\n",
    "    \"\"\"\n",
    "    return experiment[ppl].pipeline.get_model_by_name(model_name).get_number_of_trainable_vars()\n",
    "\n",
    "\n",
    "def calc_metrics(iteration, experiment, ppl, var_name):\n",
    "    \"\"\"\n",
    "    Gets variable with metrics class from the pipeline and\n",
    "    calculates F1 score.\n",
    "    \"\"\"\n",
    "    metrics = experiment[ppl].pipeline.get_variable(var_name)\n",
    "    f1 = metrics.evaluate('f1_score')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the whole research pipeline and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = (\n",
    "    Research()\n",
    "    .pipeline(root_train << eds, model_train,\n",
    "              variables=[\"loss\"], name=\"train\", dump=STR_EXEC)\n",
    "    .pipeline(root_test << eds, model_test,\n",
    "              name=\"test\", execute=STR_EXEC, dump=STR_EXEC,\n",
    "              import_from=\"train\", run=True)\n",
    "    .function(calc_metrics, returns='metrics_f1', name='metrics_f1',\n",
    "              execute=STR_EXEC, dump=STR_EXEC, ppl='test', var_name='metrics')\n",
    "    .function(get_trainable_variables, returns='trainable_variables', \n",
    "              name='trainable_variables', execute=1, dump=1, ppl='train')\n",
    "    .grid(grid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.run(n_reps=N_REPS, n_iters=ITERATIONS, workers=N_WORKERS, gpu=GPU, \n",
    "       branches=N_BRANCHES, name='ResNet_research', progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in folder ResNet_research you can find the results: loss, saved after each iteration, f1 score on the whole test set calculated each 20 epochs and number of trainable variables in each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have learned about research and experiments and now you know how to:\n",
    "* define research grid\n",
    "* use metrics in the pipeline\n",
    "* run research experiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
